{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learnの様々な分類アルゴリズムを試す\n",
    "参考URL：[機械学習 iris データセットを用いて scikit-learn の様々な分類アルゴリズムを試してみた](https://qiita.com/ao_log/items/fe9bd42fd249c2a7ee7a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# irisデータセットのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの説明\n",
    "データセットの説明は DESCR (description) から見ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの形状\n",
    "説明変数(特徴量)は 4 個で、150 個データがあります。形状は shape で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 花の種類\n",
    "目的変数は target_names に花の種類が格納されています。 3種類あります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを眺める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データフレームに変換する\n",
    "通常データ分析をする時はpandas.DataFrame(データフレーム)に変換します。<br>\n",
    "pandas(データ解析ライブラリ）により、データが扱いやすくなります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target の行が 0, 1, 2 だと分かりにくいので種類名に置き換えます。<br>\n",
    "今回は勉強会用にわかりやすさを重視して置き換えておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ざっくり眺めるには describe です。平均値、最小、最大値が分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ペアプロット\n",
    "ペアプロットします。各特徴量のペアごとに散布図を表示させることができます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類アルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "X =    # sepal length, petal length (分類が簡単な組み合わせ。100%になってしまうこともある)\n",
    "# X =   # sepal length, sepal width (分類が難しい組み合わせ。77%ほどしか精度が出ない)\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "def decision_boundary(clf, X, y, ax, title, report_type=\"classification\"):\n",
    "\n",
    "    # graph common settings\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # データを訓練データとテストデータに分ける\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "    \n",
    "    # 学習する\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "\n",
    "    # label\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('sepal length')\n",
    "    ax.set_ylabel('petal length')\n",
    "    \n",
    "    # 学習データによる精度表示\n",
    "    if report_type is \"classification\":\n",
    "#         print('train classification_report')\n",
    "        y_true = y_test\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report_dict = classification_report(y_true, y_pred,\n",
    "                                   target_names=[\"setosa\",\"versicolor\",\"virginica\"],\n",
    "                                   output_dict=True)\n",
    "        df_report = pd.DataFrame(report_dict)\n",
    "        display(df_report)\n",
    "    elif report_type is \"regression\":\n",
    "        print('Test score: {:.3f}'.format(clf.score(X_test, y_test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教師あり学習\n",
    "## k-近傍法　(k-NN)\n",
    "モデルには訓練データセットを格納するだけです。予測時は、予測したいデータポイントの近くの k 個の近傍点を確認します。その中で一番多数派のクラスを予測結果として採用します。\n",
    "\n",
    "KNeighborsClassifierを使用します。n_neighbors で予測に使用する近傍点の数を設定します。n_neighbors = 1 の場合は決定境界が鋭角になる部分もあります。数が多くなるに従いなだらかになっていき、10 になると境界が単純になりすぎて予測性能が落ちる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "for ax, n_neighbors in zip(axes, [1, 3, 6, 10]):\n",
    "    title = \"%s neighbor(s)\"% (n_neighbors)\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    decision_boundary(clf, X, y, ax, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰\n",
    "名前に回帰とあリますが、分類アルゴリズムです。\n",
    "\n",
    "LogisticRegressionを使用します。決定境界は直線になります。C は正則化の度合いを調整するパラメータです。正則化は学習時にペナルティを与えることで過学習を抑える効果があります。C を大きくすると正則化が弱くなり過学習気味になりますが、小さすぎるとデータの特徴を大雑把にしか獲得できません。\n",
    "\n",
    "ロジスティック回帰などの線形モデルは高次元(特徴量の数が多い)のデータに対して有効です。理由は高速だから、他の手法では学習できないからです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for ax, C in zip(axes, [0.01, 1, 100]):\n",
    "    title = \"C=%s\"% (C)\n",
    "    clf = LogisticRegression(C=C, solver='lbfgs', multi_class='auto')\n",
    "    decision_boundary(clf, X, y, ax, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C=L2正則化のコスト関数の重み係数<br>\n",
    "C=100にすると精度が100%に達することがあるが、おそらくオーバーフィッティングしているのでよろしくない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形サポートベクタマシン\n",
    "LinearSVCを使用します。線形なので、決定境界も直線になります。<br>\n",
    "C はロジスティック回帰同様、正則化の度合いを調整するパラメータです。<br>\n",
    "そのまま生データを使用すると精度が出ないため、StandardScaler()で標準化を行っています<br>\n",
    "標準化を行うことでデータの平均は０に、分散は１になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for ax, C in zip(axes, [0.01, 1, 100]):\n",
    "    title = \"C=%s\"% (C)\n",
    "    clf = LinearSVC(C=C)\n",
    "    decision_boundary(clf, X_std, y, ax, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カーネル法を用いたサポートベクタマシン\n",
    "SVM とも呼ばれます。線形カーネルベクタマシンと比べて、非線形な分離が可能です。<br>\n",
    "<br>\n",
    "SVCを使用します。C は正則化のパラメータです、gamma は訓練データの影響が及ぶ範囲で、小さいと遠くまで、大きいと近くになります。よって、gamma が大きすぎると過学習になり、小さすぎると大雑把にしかデータの特徴を獲得できません。<br>\n",
    "<br>\n",
    "パラメータの説明は RBF SVM parametersのページにも書かれています。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "for ax_row, C in zip(axes, [0.01, 1, 100]):\n",
    "    for ax, gamma in zip(ax_row, [0.1, 1, 10]):\n",
    "        title = \"C=%s, gamma=%s\"% (C, gamma)\n",
    "        clf = SVC(C=C, gamma=gamma)\n",
    "        decision_boundary(clf, X, y, ax, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木\n",
    "質問を通してデータを分類する手法です。あやめの場合は、花弁の長さが何センチ以上 or 未満、ガクの長さが何センチ以上 or 未満・・・と質問を繰り返すことで品種を特定していきます。\n",
    "\n",
    "DecisionTreeRegressor を使用します。max_depth がツリーの深さで、質問数になります。多ければいいわけでもなく、訓練データに過剰適合するので過学習となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "\n",
    "for ax, max_depth in zip(axes, [1, 3, 5, 8]):\n",
    "    title = \"max_depth=%s\"% (max_depth)\n",
    "    clf = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    decision_boundary(clf, X, y, ax, title, report_type=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムフォレスト\n",
    "ランダムフォレストは異なる決定木をたくさん作ります。<br>\n",
    "全ての決定木で予測した結果からもっとも確率が高くなるラベルを正解とするものです。<br>\n",
    "個々の木だと過剰適合しているかもしれないですが、多くの結果を集約することで過学習を抑制する効果があります。RandomForestClassifier を使います。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "decision_boundary(clf, X, y, ax, \"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配ブースティング回帰木\n",
    "勾配ブースティング回帰木(GBDT = Gradient Boosting Decision Tree)もたくさんの決定木を作るのですが、一つ前の決定木の予測値と正解のズレを修正するように次の木を作っていくのだそうです。GradientBoostingClassifier を使います。<br>\n",
    "こちらの方がランダムフォレストよりモデル構築に時間がかかったり、パラメータ設定のチューニングが大変らしいのですが、その分予測性能がよくなるのだそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "clf = GradientBoostingClassifier()\n",
    "decision_boundary(clf, X, y, ax, \"GradientBoostingClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "scikit-learn にはニューラルネットワークのライブラリも実装されています。<br>\n",
    "なお、scikit-learn は GPU に対応していない こともあり、ディープラーニングをやりたい場合は他のライブラリを選定した方がいいと思います。<br>\n",
    "\n",
    "前置きはさておき、MLPClassifier を使います。<br>\n",
    "隠れ層 15 個で計算した結果を図示しますが、同じパラメータでも決定境界が異なっています。<br>\n",
    "そうなる理由は、それぞれ異なる初期状態から学習を開始しているためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "for ax, n in zip(axes, [15, 15, 15, 15]):\n",
    "    title = \"\"\n",
    "    clf = MLPClassifier(hidden_layer_sizes=[n, n])\n",
    "    decision_boundary(clf, X, y, ax, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教師なし学習\n",
    "## k-means\n",
    "教師データなしで分類する手法です。KMeans を使います。n_clustersで何個に分類するかを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "for ax, n_clusters in zip(axes, [2, 3, 4, 5]):\n",
    "    title = \"n_clusters=%s\"% (n_clusters)\n",
    "    clf = KMeans(n_clusters=n_clusters)\n",
    "    decision_boundary(clf, X, y, ax, title, report_type=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
